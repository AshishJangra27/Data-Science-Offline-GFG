{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xO9SqbZpXk9"
      },
      "source": [
        "# Introduction\n",
        "In this guide, we'll implement **logistic regression** from scratch using **gradient descent**. Starting with **dataset loading**, we'll cover the **mathematical foundations** and step-by-step code implementation.\n",
        "\n",
        "The goal is to understand how **logistic regression** works, how **gradient descent** optimizes model parameters, and how to build it without **high-level machine learning libraries**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lzvDNCBpXk-"
      },
      "source": [
        "### Table of Contents\n",
        "\n",
        "1. **Importing Libraries**\n",
        "   - Setting up the necessary libraries for data manipulation, model implementation, and visualization.\n",
        "2. **Loading and Exploring the Dataset**\n",
        "   - Understanding the structure of the dataset and initial data exploration.\n",
        "3. **Preparing the Data**\n",
        "   - Preprocessing the data by scaling features and splitting into training and testing sets.\n",
        "4. **Initializing Parameters**\n",
        "   - Defining the initial parameters for the model, including weights and bias.\n",
        "5. **Defining the Sigmoid Function**\n",
        "   - Implementing the model's prediction function using the sigmoid activation.\n",
        "6. **Defining the Cost Function**\n",
        "   - Formulating the cost function to measure the accuracy of predictions against actual values using the log loss formula.\n",
        "7. **Computing the Gradients**\n",
        "   - Calculating the gradients for weights and bias to optimize the cost function.\n",
        "8. **Updating Parameters Using Gradient Descent**\n",
        "   - Applying gradient descent to adjust parameters and minimize the cost function.\n",
        "9. **Training the Model**\n",
        "   - Training the model using the data and updating parameters through iterative optimization.\n",
        "10. **Evaluating Model Performance with Test Data**\n",
        "    - Assessing the model's performance using test data and relevant metrics.\n",
        "11. **Conclusion**\n",
        "    - Summarizing the key findings and insights from the model implementation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QV6hYWPpXk_"
      },
      "source": [
        "# 1. Importing Libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code imports essential libraries for linear regression and dataset loading:\n",
        "\n",
        "- **numpy**: For numerical computing and array manipulation.\n",
        "- **load_breast_canver**: Loads the Breast Cancer dataset for classification tasks.\n",
        "- **matplotlib.pyplot**: For visualizations such as loss curves and predictions."
      ],
      "metadata": {
        "id": "zh3io1DPp1rl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "YtD64TMqpXk_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvyH0BFlpXlA"
      },
      "source": [
        "# 2. Loading and Exploring the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **`data = load_breast_cancer()`**:\n",
        "   - Loads the Breast Cancer dataset from `sklearn.datasets`.\n",
        "\n",
        "2. **`X = data.data`**:\n",
        "   - Extracts feature matrix (e.g., mean radius, texture, area) for each sample.\n",
        "\n",
        "3. **`y = data.target`**:\n",
        "   - Extracts target labels:\n",
        "     - `0`: Malignant\n",
        "     - `1`: Benign"
      ],
      "metadata": {
        "id": "-jweSwEpqz-h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "PFLedu-2pXlA"
      },
      "outputs": [],
      "source": [
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeaTY0XPpXlA"
      },
      "source": [
        "# 3. Preparing the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **`from sklearn.preprocessing import StandardScaler`**:\n",
        "   - Imports the `StandardScaler` for feature standardization.\n",
        "\n",
        "2. **`from sklearn.model_selection import train_test_split`**:\n",
        "   - Imports the `train_test_split` function to split the dataset into training and testing sets.\n",
        "\n",
        "3. **`scaler = StandardScaler()`**:\n",
        "   - Initializes a scaler to standardize features by removing the mean and scaling to unit variance.\n",
        "\n",
        "4. **`X = scaler.fit_transform(X)`**:\n",
        "   - Standardizes the feature matrix `X`.\n",
        "\n",
        "5. **`X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)`**:\n",
        "   - Splits the dataset into training and testing sets:\n",
        "     - 80% for training (`X_train`, `y_train`).\n",
        "     - 20% for testing (`X_test`, `y_test`).\n",
        "   - Ensures reproducibility with `random_state=42`."
      ],
      "metadata": {
        "id": "tqLkGoZYrHFh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "_l9wy2zbpXlB"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtaFX_4SpXlB"
      },
      "source": [
        "# 4. Initializing Parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **`weights = np.zeros(X_train.shape[1])`**:\n",
        "   - Initializes the weight vector for the model.\n",
        "   - The weights have the same number of elements as the features (`X_train.shape[1]`).\n",
        "   - All weights are initially set to `0`.\n",
        "\n",
        "2. **`bias = 0`**:\n",
        "   - Initializes the bias term to `0`.\n",
        "   - This is a scalar value added to the model's predictions."
      ],
      "metadata": {
        "id": "1pIjoG7UrY77"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2F8AbCyK56RA",
        "outputId": "e3eea095-11e2-4be4-e75a-8ac02f237e34"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "K1BJaWlRpXlB"
      },
      "outputs": [],
      "source": [
        "weights = np.zeros(X_train.shape[1])\n",
        "bias = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvKRtEqppXlB"
      },
      "source": [
        "# 5. Defining the Sigmoid Function"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **`def sigmoid(z):`**:\n",
        "   - Defines the sigmoid activation function.\n",
        "\n",
        "2. **`return 1 / (1 + np.exp(-z))`**:\n",
        "   - Computes the sigmoid of `z` using the formula:\n",
        "\n",
        "   $$\n",
        "   \\large \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "   $$\n",
        "\n",
        "The sigmoid function maps input `z` to a range between 0 and 1, representing probabilities in binary classification tasks.\n"
      ],
      "metadata": {
        "id": "aJEzraq-rieD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "N1SJKzvfpXlB"
      },
      "outputs": [],
      "source": [
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qouuLBZ_pXlB"
      },
      "source": [
        "# 6. Defining the Cost Function"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **`def compute_cost(X, y, weights, bias):`**:\n",
        "   - Defines a function to compute the cost (log loss) for logistic regression.\n",
        "\n",
        "2. **`m = len(y)`**:\n",
        "   - Determines the number of training samples (`m`).\n",
        "\n",
        "3. **`predictions = sigmoid(np.dot(X, weights) + bias)`**:\n",
        "   - Calculates the predicted probabilities:\n",
        "     - Uses the sigmoid function on the linear combination of features, weights, and bias.\n",
        "     - Formula:\n",
        "\n",
        "  $$\n",
        "  \\large \\ z =  \\left( \\sum_{j=1}^{n} w_j x_j^{(i)} + b \\right)\n",
        "  $$\n",
        "  \n",
        "  \n",
        "\n",
        "  $$\n",
        "  \\large \\hat{y}_{(i)} = \\sigma \\left(\\ z \\right)\n",
        "  $$\n",
        "  \n",
        "\n",
        "\n",
        "4. **`cost = -1 / m * np.sum(y * np.log(predictions) + (1 - y) * np.log(1 - predictions))`**:\n",
        "   - Computes the cost function (log loss):\n",
        "     - Formula:\n",
        "\n",
        "  $$\n",
        "  \\large J = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log(\\hat{y}_{(i)}) + (1 - y^{(i)}) \\log(1 - \\hat{y}_{(i)}) \\right]\n",
        "  $$\n",
        "\n",
        "5. **`return cost`**:\n",
        "   - Returns the computed cost value.\n",
        "\n",
        "\n",
        "Calculates the log loss, which measures the difference between predicted probabilities and actual labels. Lower cost indicates better model performance.\n"
      ],
      "metadata": {
        "id": "NNVX-PzqsgHB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "4hCTifLzpXlB"
      },
      "outputs": [],
      "source": [
        "def compute_cost(X, y, weights, bias):\n",
        "    m = len(y)\n",
        "    predictions = sigmoid(np.dot(X, weights) + bias)\n",
        "    cost = -1 / m * np.sum(y * np.log(predictions) + (1 - y) * np.log(1 - predictions))\n",
        "    return cost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QCKZ1TupXlC"
      },
      "source": [
        "# 7. Computing the Gradients"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **`def compute_gradients(X, y, predictions):`**:\n",
        "   - Defines a function to compute gradients of the cost function with respect to weights and bias.\n",
        "\n",
        "2. **`m = X.shape[0]`**:\n",
        "   - Retrieves the number of training samples (`m`).\n",
        "\n",
        "3. **`dz = predictions - y`**:\n",
        "   - Computes the error between predicted probabilities (`predictions`) and true labels (`y`).\n",
        "\n",
        "4. **`dw = 1 / m * np.dot(X.T, dz)`**:\n",
        "   - Computes the gradient of the cost function with respect to weights (`dw`).\n",
        "   - Formula:\n",
        "\n",
        "  $$\n",
        "  \\large \\frac{\\partial J}{\\partial w_j} = \\frac{1}{m} \\sum_{i=1}^{m} \\left( \\hat{y}_{(i)} - y^{(i)} \\right) x_j^{(i)}\n",
        "  $$\n",
        "\n",
        "5. **`db = 1 / m * np.sum(dz)`**:\n",
        "   - Computes the gradient of the cost function with respect to the bias (`db`).\n",
        "   - Formula:\n",
        "\n",
        "  $$\n",
        "  \\large \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^{m} \\left( \\hat{y}_{(i)} - y^{(i)} \\right)\n",
        "  $$\n",
        "\n",
        "6. **`return dw, db`**:\n",
        "   - Returns the computed gradients for weights (`dw`) and bias (`db`).\n",
        "\n",
        "Computes the gradients needed for updating weights and bias during model training using gradient descent.\n"
      ],
      "metadata": {
        "id": "QDhrlebbtdhp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "6ZhJBVbppXlC"
      },
      "outputs": [],
      "source": [
        "def compute_gradients(X, y, predictions):\n",
        "\n",
        "    m = X.shape[0]\n",
        "    dz = predictions - y\n",
        "\n",
        "    dw = 1 / m * np.dot(X.T, dz)\n",
        "    db = 1 / m * np.sum(dz)\n",
        "\n",
        "    return dw, db"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-KWcJ-npXlC"
      },
      "source": [
        "# 8. Updating Parameters Using Gradient Descent"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **`def update_parameters(weights, bias, dw, db, learning_rate):`**:\n",
        "   - Defines a function to update the model's weights and bias using gradients and the learning rate.\n",
        "\n",
        "2. **`weights -= learning_rate * dw`**:\n",
        "   - Updates the weights by subtracting the product of the learning rate and the gradient of weights (`dw`).\n",
        "   - Formula:\n",
        "  $$\n",
        "  \\large w_j = w_j - \\alpha \\frac{\\partial J}{\\partial w_j}\n",
        "  $$\n",
        "     Where:\n",
        "\n",
        "  $$\n",
        "  \\alpha = \\text{learning rate}\n",
        "  $$\n",
        "\n",
        "\n",
        "3. **`bias -= learning_rate * db`**:\n",
        "   - Updates the bias by subtracting the product of the learning rate and the gradient of the bias (`db`).\n",
        "   - Formula:\n",
        "  $$\n",
        "  \\large b = b - \\alpha \\frac{\\partial J}{\\partial b}\n",
        "  $$\n",
        "\n",
        "4. **`return weights, bias`**:\n",
        "   - Returns the updated weights and bias.\n",
        "\n",
        "Applies gradient descent to iteratively update weights and bias, minimizing the cost function and improving model performance.\n"
      ],
      "metadata": {
        "id": "9L7kMgkzuBZb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "TlylnE7gpXlC"
      },
      "outputs": [],
      "source": [
        "def update_parameters(weights, bias, dw, db, learning_rate):\n",
        "\n",
        "    weights -= learning_rate * dw\n",
        "    bias -= learning_rate * db\n",
        "    return weights, bias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mezr8eh6pXlC"
      },
      "source": [
        "# 9. Training the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **`def train(X, y, weights, bias, learning_rate, iterations):`**\n",
        "   - Trains a logistic regression model using gradient descent over a given number of iterations.\n",
        "\n",
        "2. **`cost_history = []`**\n",
        "   - Initializes a list to store the cost value at every 100 iterations for monitoring training progress.\n",
        "\n",
        "3. **`for i in range(iterations):`**\n",
        "   - Iteratively updates model parameters using the following steps:\n",
        "\n",
        "   - **Predictions**:\n",
        "     - `predictions = sigmoid(np.dot(X, weights) + bias)`\n",
        "     - Calculates the predicted probabilities for the current weights and bias.\n",
        "\n",
        "   - **Cost Calculation**:\n",
        "     - `cost = compute_cost(X, y, weights, bias)`\n",
        "     - Computes the cost function (log loss) to evaluate the model's performance.\n",
        "\n",
        "   - **Gradients**:\n",
        "     - `dw, db = compute_gradients(X, y, predictions)`\n",
        "     - Calculates the gradients of the cost function with respect to weights (`dw`) and bias (`db`).\n",
        "\n",
        "   - **Parameter Updates**:\n",
        "     - `weights, bias = update_parameters(weights, bias, dw, db, learning_rate)`\n",
        "     - Updates weights and bias using the learning rate and computed gradients.\n",
        "\n",
        "   - **Cost Logging**:\n",
        "     - Every 100 iterations (`if i % 100 == 0`):\n",
        "       - Appends the current cost to `cost_history`.\n",
        "       - Prints the cost value for monitoring progress.\n",
        "\n",
        "4. **`return weights, bias, cost_history`**\n",
        "   - Returns the optimized weights, bias, and the cost history.\n",
        "\n",
        "**Model Training**\n",
        "\n",
        "1. **`weights, bias, _ = train(X_train, y_train, weights, bias, 0.01, 1000)`**\n",
        "   - Trains the logistic regression model using the training data (`X_train`, `y_train`).\n",
        "   - Parameters:\n",
        "     - Learning rate: `0.01`\n",
        "     - Iterations: `1000`\n",
        "   - Outputs:\n",
        "     - Optimized weights and bias after training.\n",
        "     - Logs cost values every 100 iterations.\n",
        "\n",
        "- Trains the model by minimizing the cost function using gradient descent.\n",
        "- Outputs optimized model parameters to make predictions."
      ],
      "metadata": {
        "id": "bJbpNTdyuyDf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-RHP23UvpXlC",
        "outputId": "5659213e-11be-40e8-c649-f1f28d824214"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cost after iteration 0: 0.6931\n",
            "Cost after iteration 100: 0.2522\n",
            "Cost after iteration 200: 0.1897\n",
            "Cost after iteration 300: 0.1615\n",
            "Cost after iteration 400: 0.1448\n",
            "Cost after iteration 500: 0.1336\n",
            "Cost after iteration 600: 0.1253\n",
            "Cost after iteration 700: 0.1190\n",
            "Cost after iteration 800: 0.1139\n",
            "Cost after iteration 900: 0.1097\n"
          ]
        }
      ],
      "source": [
        "def train(X, y, weights, bias, learning_rate, iterations):\n",
        "\n",
        "    cost_history = []\n",
        "\n",
        "    for i in range(iterations):\n",
        "\n",
        "        predictions = sigmoid(np.dot(X, weights) + bias)\n",
        "        cost = compute_cost(X, y, weights, bias)\n",
        "\n",
        "        dw, db = compute_gradients(X, y, predictions)\n",
        "\n",
        "        weights, bias = update_parameters(weights, bias, dw, db, learning_rate)\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            cost_history.append(cost)\n",
        "            print(f'Cost after iteration {i}: {cost:.4f}')\n",
        "    return weights, bias, cost_history\n",
        "\n",
        "weights, bias, _ = train(X_train, y_train, weights, bias, 0.01, 1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQfQfV-VpXlC"
      },
      "source": [
        "# 10. Evaluating Model Performance with Test Data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **`final_predictions = sigmoid(np.dot(X_test, weights) + bias)`**:\n",
        "   - Computes the predicted probabilities for the test set (`X_test`) using the trained model.\n",
        "\n",
        "2. **`accuracy = np.mean((final_predictions > 0.5) == y_test)`**:\n",
        "   - Calculates the accuracy of the model:\n",
        "     - Converts predicted probabilities to binary labels using a threshold of `0.5`.\n",
        "     - Compares predicted labels with actual labels (`y_test`) to calculate the proportion of correct predictions.\n",
        "\n",
        "3. **`print(f'Test accuracy: {accuracy * 100:.2f}%')`**:\n",
        "   - Prints the test accuracy as a percentage.\n",
        "\n",
        "- Trains the logistic regression model using the training dataset.\n",
        "- Evaluates the model's performance on unseen test data, reporting its classification accuracy.\n",
        "\n",
        "\n",
        "**Formula Summary**\n",
        "\n",
        "\n",
        "- Prediction for Test Data:\n",
        "  $$\n",
        "  \\large \\hat{y}_{(i)} = \\sigma \\left( \\sum_{j=1}^{n} w_j x_j^{(i)} + b \\right)\n",
        "  $$\n",
        "\n",
        "- Accuracy:\n",
        "  $$\n",
        "  \\small \\text{Accuracy} = \\frac{\\text{Number of Correct Predictions}}{\\text{Total Predictions}} \\times 100\n",
        "  $$\n",
        "\n",
        "\n",
        "- The model outputs the test accuracy, e.g.,:"
      ],
      "metadata": {
        "id": "AVamv9mFv4aE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(sigmoid(np.dot(X_test, weights) + bias) > .5) == y_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3yAebC799uWo",
        "outputId": "1e951ff7-7b50-42bc-e4d4-13d546853a32"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "        True,  True, False,  True,  True,  True,  True,  True,  True,\n",
              "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "        True,  True,  True,  True,  True,  True])"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CcBwrwFD9uR-",
        "outputId": "2b1770c4-2075-4e11-b492-8a39674a2878"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,\n",
              "       0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n",
              "       0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,\n",
              "       0, 1, 1, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZ95_HN6pXlC",
        "outputId": "17bd88bf-e73a-4e66-b526-8bdbc8ff4eef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9912280701754386\n",
            "Test accuracy: 99.12%\n"
          ]
        }
      ],
      "source": [
        "final_predictions = sigmoid(np.dot(X_test, weights) + bias)\n",
        "\n",
        "accuracy = np.mean((final_predictions > 0.5) == y_test)\n",
        "\n",
        "print(accuracy)\n",
        "\n",
        "print(f'Test accuracy: {accuracy * 100:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = {'weights' : weights, 'bias' : bias}"
      ],
      "metadata": {
        "id": "6yt6lTyEAiqD"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import numpy as np\n",
        "\n",
        "model = {'weights': weights, 'bias': bias}\n",
        "model['weights'] = model['weights'].tolist()\n",
        "\n",
        "with open('sample_data/model.json', 'w') as f:\n",
        "    json.dump(model, f, indent=4)"
      ],
      "metadata": {
        "id": "7gUspGOmAwlP"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tKJuknTm8f4C",
        "outputId": "1ac422f7-35aa-4605-c0f0-326ac9e67052"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'weights': array([-0.37522937, -0.36006938, -0.37107979, -0.37113421, -0.15591433,\n",
              "        -0.13275051, -0.28928913, -0.39138848, -0.09339357,  0.1497156 ,\n",
              "        -0.32934913, -0.01345871, -0.27531747, -0.29989143, -0.00308254,\n",
              "         0.11363218,  0.093067  , -0.04782715,  0.0757411 ,  0.19622723,\n",
              "        -0.43974159, -0.43492227, -0.41869554, -0.41332874, -0.30778806,\n",
              "        -0.20946152, -0.29105911, -0.39796251, -0.30886471, -0.09375225]),\n",
              " 'bias': 0.3265103861459601}"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bias"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uNImdJ1UAVQy",
        "outputId": "7d123085-3719-4167-adc4-274a7ac9d5ca"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3265103861459601"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNLyVfo1pXlC"
      },
      "source": [
        "# 11. Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this project, we successfully implemented a **logistic regression model** from scratch using gradient descent.\n",
        "\n",
        "\n",
        "## Key Findings:\n",
        "- The logistic regression model was successfully trained and optimized using gradient descent.\n",
        "- The final learned weights and bias demonstrated effective convergence.\n",
        "- Test accuracy provided a measure of the model's performance on unseen data.\n",
        "\n",
        "---\n",
        "\n",
        "## Benefits of Logistic Regression:\n",
        "- **Probabilistic Predictions**: Outputs probabilities for binary classification, making it interpretable.\n",
        "- **Simplicity**: Easy to implement and computationally efficient.\n",
        "- **Linear Decision Boundary**: Works well for linearly separable data.\n",
        "\n",
        "---\n",
        "\n",
        "## Drawbacks of Logistic Regression:\n",
        "- **Assumption of Linearity**: Assumes linearity in the relationship between input features and the log-odds of the target variable.\n",
        "- **Limited to Binary Classification**: Needs extensions for multi-class problems (e.g., one-vs-rest, softmax regression).\n",
        "- **Sensitivity to Outliers**: Outliers can influence the decision boundary significantly.\n",
        "\n",
        "---\n",
        "\n",
        "## Conclusion:\n",
        "Logistic regression is a robust, interpretable model for binary classification tasks with linearly separable data. While its simplicity is an advantage, it may require feature engineering or advanced techniques for non-linear problems or datasets with significant noise. For more complex scenarios, models like support vector machines or neural networks can provide better performance."
      ],
      "metadata": {
        "id": "IsNRGXthx1Vb"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1984eM9uCQFh"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}