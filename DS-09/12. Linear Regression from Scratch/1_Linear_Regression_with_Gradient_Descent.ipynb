{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJ78tTOX-E0n"
      },
      "source": [
        "# Introduction\n",
        "In this guide, we'll implement **linear regression** from scratch using **gradient descent**. Starting with **dataset loading**, we'll cover the **mathematical foundations** and step-by-step code implementation.\n",
        "\n",
        "The goal is to understand how **linear regression** works, how **gradient descent** optimizes model parameters, and how to build it without **high-level machine learning libraries**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-wTX3eq-E0p"
      },
      "source": [
        "### Table of Contents\n",
        "\n",
        "1. **Importing Libraries**  \n",
        "   Setting up the necessary libraries for data manipulation, model implementation, and visualization.\n",
        "\n",
        "2. **Loading and Exploring the Dataset**  \n",
        "   Understanding the structure of the dataset and initial data exploration.\n",
        "\n",
        "3. **Preparing the Data**  \n",
        "   Preprocessing the data by scaling features and splitting into training and testing sets.\n",
        "\n",
        "4. **Initializing Parameters**  \n",
        "   Defining the initial parameters for the model, including weights and bias.\n",
        "\n",
        "5. **Defining the Prediction Function**  \n",
        "   Implementing the model's prediction function to make estimates based on input data.\n",
        "\n",
        "6. **Defining the Cost Function**  \n",
        "   Formulating the cost function to measure the accuracy of predictions against actual values.\n",
        "\n",
        "7. **Computing the Gradients**  \n",
        "   Calculating the gradients for weights and bias to optimize the cost function.\n",
        "\n",
        "8. **Updating Parameters Using Gradient Descent**  \n",
        "   Applying gradient descent to adjust parameters and minimize the cost function.\n",
        "\n",
        "9. **Training the Model**  \n",
        "   Training the model using the data and updating parameters through iterative optimization.\n",
        "\n",
        "10. **Evaluating Model Performance with Test Data**  \n",
        "    Assessing the model's performance using test data and relevant metrics.\n",
        "\n",
        "11. **Conclusion**  \n",
        "    Summarizing the key findings and insights from the model implementation.\n",
        "\n",
        "12. **Comparison with Sklearn Linear Regression**  \n",
        "    Side by side comparison of the algorithm that we've written with the algorithms predefined in sklearn to check performance\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnlYUqZm-E0p"
      },
      "source": [
        "# 1. Importing Libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code imports essential libraries for linear regression and dataset loading:\n",
        "\n",
        "- **numpy**: For numerical computing and array manipulation.\n",
        "- **load_diabetes**: Loads the Diabetes dataset for regression tasks.\n",
        "- **matplotlib.pyplot**: For visualizations such as loss curves and predictions."
      ],
      "metadata": {
        "id": "kCTn8GRJMJp5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "qa9RvNW--E0q"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_diabetes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nmrg0hp-E0q"
      },
      "source": [
        "# 2. Loading and Exploring the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code loads the **Diabetes** dataset and prints:\n",
        "\n",
        "- **X**: Feature matrix (442 samples, 10 features).\n",
        "- **y**: Target vector (442 values, diabetes progression).\n",
        "\n",
        "It also displays the feature names, the first five samples of X, and the first five target values.\n"
      ],
      "metadata": {
        "id": "IplIE9GDMMgv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "liCDjqwm-E0q",
        "outputId": "5fb3ba4d-8a23-4bd2-fd02-ab59544907f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature names: ['age', 'sex', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6']\n",
            "First five target values: [151.  75. 141. 206. 135.]\n"
          ]
        }
      ],
      "source": [
        "data = load_diabetes()\n",
        "\n",
        "X = data.data         # Feature matrix (shape: [442, 10])\n",
        "y = data.target       # Target vector (shape: [442,])\n",
        "\n",
        "print('Feature names:', data.feature_names)\n",
        "print('First five target values:', y[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSo7mxNN-E0r"
      },
      "source": [
        "# 3. Preparing the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code standardizes the features and splits the dataset into training and testing sets:\n",
        "\n",
        "- **StandardScaler**: Standardizes the feature matrix by removing the mean and scaling to unit variance.\n",
        "- **train_test_split**: Splits the dataset into training (80%) and testing (20%) sets.\n",
        "\n",
        "The feature matrix **X** is transformed, and the dataset is divided into **X**, **X_test**, **y**, and **y_test**.\n"
      ],
      "metadata": {
        "id": "AqSNezZ6MQcz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "e77PjPjb-E0r"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "feature_scaler = StandardScaler()\n",
        "target_scaler = StandardScaler()\n",
        "\n",
        "X = feature_scaler.fit_transform(X)\n",
        "\n",
        "# y = y.reshape(-1, 1)\n",
        "# y = target_scaler.fit_transform(y)\n",
        "# y = y.ravel()\n",
        "\n",
        "X, X_test, y, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQdNE_gf-E0r"
      },
      "source": [
        "# 4. Initializing Parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code initializes the parameters for the linear regression model:\n",
        "\n",
        "- **m, n**: The shape of the feature matrix **X**, where **m** is the number of samples (442) and **n** is the number of features (10).\n",
        "- **w**: The weight vector, initialized to zeros with shape **[10,]**.\n",
        "- **b**: The bias term, initialized to **0**.\n"
      ],
      "metadata": {
        "id": "h8_4ycnoMaS8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "HFJqTKZu-E0r"
      },
      "outputs": [],
      "source": [
        "m, n = X.shape   # m = 442, n = 10\n",
        "w = np.zeros(n)  # Weight vector (shape: [10,])\n",
        "b = 0            # Bias term (scalar)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_PZqbPD-E0r"
      },
      "source": [
        "# 5. Defining the Prediction Function"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The prediction function is given by:\n",
        "\n",
        "$$\n",
        "\\large \\hat{y}_{(i)} = \\sum_{j=1}^{n} w_j x_j^{(i)} + b\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "**Where:**\n",
        "\n",
        " $$ \\hat{y}_{(i)} = \\text{predicted target value for the } i\\text{-th sample.} $$\n",
        " $$ w_j = \\text{weight for the } j\\text{-th feature.} $$\n",
        " $$ x_j^{(i)} = \\text{value of the } j\\text{-th feature for the } i\\text{-th sample.} $$"
      ],
      "metadata": {
        "id": "ZqRBy6OXMAe7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "JdTZTiRz-E0s"
      },
      "outputs": [],
      "source": [
        "def predict(X, w, b):\n",
        "    return np.dot(X, w) + b"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code defines the **predict** function, which calculates the predicted target values for each sample using the feature matrix **X**, weight vector **w**, and bias term **b**:\n",
        "\n",
        "- **X**: The feature matrix (shape: [m, n]), where **m** is the number of samples and **n** is the number of features.\n",
        "- **w**: The weight vector (shape: [n,]), which contains the learned weights for each feature.\n",
        "- **b**: The bias term (scalar), which is added to the weighted sum.\n",
        "\n",
        "The function uses the following formula to compute the predictions:\n",
        "\n",
        "$$\n",
        "\\hat{y} = Xw + b\n",
        "$$"
      ],
      "metadata": {
        "id": "ruQB_D9KLo_i"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huel8spq-E0s"
      },
      "source": [
        "# 6. Defining the Cost Function"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The cost function measures how well the model's predictions match the actual target values and is defined as:\n",
        "\n",
        "$$\n",
        "\\large J(w, b) = \\frac{1}{2m} \\sum_{i=1}^{m} \\left( \\hat{y}_{(i)} - y_{(i)} \\right)^2\n",
        "$$\n",
        "\n",
        "Where **m** is the number of training examples.\n"
      ],
      "metadata": {
        "id": "RQDmaCDtM0-R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "hjdqJ4Xq-E0s"
      },
      "outputs": [],
      "source": [
        "def compute_cost(X, y, w, b):\n",
        "    m = len(y)\n",
        "    y_pred = predict(X, w, b)\n",
        "    cost = (1 / (2 * m)) * np.sum((y_pred - y) ** 2)\n",
        "\n",
        "    return cost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsa2rfBf-E0s"
      },
      "source": [
        "# 7. Computing the Gradients"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradients are computed to update the weights and bias during training. The partial derivatives of the cost function with respect to each weight and the bias are:\n",
        "\n",
        "$$\n",
        "\\large \\frac{\\partial J}{\\partial w_j} = \\frac{1}{m} \\sum_{i=1}^{m} \\left( \\hat{y}_{(i)} - y_{(i)} \\right) x_j^{(i)}\n",
        "$$\n",
        "\n",
        ".\n",
        "\n",
        "$$\n",
        "\\large \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^{m} \\left( \\hat{y}_{(i)} - y_{(i)} \\right)\n",
        "$$"
      ],
      "metadata": {
        "id": "G8F0Bpe9NJZZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "1q72iWAr-E0s"
      },
      "outputs": [],
      "source": [
        "def compute_gradients(X, y, w, b):\n",
        "\n",
        "    m = len(y)\n",
        "    y_pred = predict(X, w, b)\n",
        "    error = y_pred - y\n",
        "\n",
        "    dw = (1 / m) * np.dot(X.T, error)\n",
        "    db = (1 / m) * np.sum(error)\n",
        "\n",
        "    return dw, db"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moKRi5uP-E0s"
      },
      "source": [
        "# 8. Updating Parameters Using Gradient Descent"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parameters are updated using the learning rate and the gradients calculated:\n",
        "\n",
        "$$\n",
        "\\large w := w - \\alpha \\frac{\\partial J}{\\partial w}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\large b := b - \\alpha \\frac{\\partial J}{\\partial b}\n",
        "$$\n",
        "\n",
        "**Where:**\n",
        "\n",
        "$$\n",
        "\\alpha = \\text{learning rate}\n",
        "$$"
      ],
      "metadata": {
        "id": "ANmNKcQfNyBX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "nKOf5gdI-E0s"
      },
      "outputs": [],
      "source": [
        "def update_parameters(w, b, dw, db, learning_rate):\n",
        "\n",
        "    w = w - learning_rate * dw\n",
        "    b = b - learning_rate * db\n",
        "\n",
        "    return w, b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ycl4HylF-E0s"
      },
      "source": [
        "# 9. Training the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code performs the training of the linear regression model using gradient descent. It iteratively updates the model's parameters (weights and bias) and tracks the cost function:\n",
        "\n",
        "- **learning_rate**: The step size for updating the parameters during each iteration. Set to **0.01**.\n",
        "- **num_iterations**: The number of iterations for the gradient descent process. Set to **1000**.\n",
        "- **cost_history**: A list that stores the cost value at each iteration to track the convergence of the model.\n",
        "\n",
        "The training loop performs the following steps for each iteration:\n",
        "\n",
        "1. **Prediction**: Uses the `predict` function to compute the predicted target values (**y_pred**).\n",
        "2. **Cost Calculation**: Computes the cost (error) using the `compute_cost` function.\n",
        "3. **Store Cost**: Appends the cost to the **cost_history** list for tracking.\n",
        "4. **Gradient Calculation**: Computes the gradients for the weights and bias using `compute_gradients`.\n",
        "5. **Parameter Update**: Updates the weights and bias using the `update_parameters` function with the calculated gradients and learning rate.\n",
        "6. **Iteration Logging**: Every 100th iteration, prints the current iteration number and the corresponding cost."
      ],
      "metadata": {
        "id": "iTjQvxeCN7lp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HXGFqpFg-E0t",
        "outputId": "2d11e506-3362-483e-e009-600757b22b49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0: Cost = 14855.6615\n",
            "Iteration 1000: Cost = 3043.6820\n",
            "Iteration 2000: Cost = 1665.4576\n",
            "Iteration 3000: Cost = 1479.7297\n",
            "Iteration 4000: Cost = 1453.4973\n",
            "Iteration 5000: Cost = 1449.3915\n",
            "Iteration 6000: Cost = 1448.5068\n",
            "Iteration 7000: Cost = 1448.1322\n",
            "Iteration 8000: Cost = 1447.8521\n",
            "Iteration 9000: Cost = 1447.5957\n"
          ]
        }
      ],
      "source": [
        "w = np.zeros(n)\n",
        "b = 0\n",
        "\n",
        "learning_rate = .001\n",
        "num_iterations = 10000\n",
        "cost_history = []\n",
        "\n",
        "parameters = {}\n",
        "\n",
        "for i in range(num_iterations):\n",
        "\n",
        "    y_pred = predict(X, w, b)\n",
        "    cost = compute_cost(X, y, w, b)\n",
        "    dw, db = compute_gradients(X, y, w, b)\n",
        "    w, b = update_parameters(w, b, dw, db, learning_rate)\n",
        "\n",
        "    if i % 1000 == 0:\n",
        "      cost_history.append(cost)\n",
        "      print(f'Iteration {i}: Cost = {cost:.4f}')\n",
        "\n",
        "      parameters = {'weights': w.tolist(), 'bias': b}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2w6DM956-E0t"
      },
      "source": [
        "# 10. Evaluating Model Performance with Test Data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code evaluates the performance of the trained linear regression model by calculating the final cost and Mean Squared Error (MSE), as well as printing the final learned parameters (weights and bias):\n",
        "\n",
        "- **y_pred**: The predicted target values obtained from the `predict` function, using the final values of weights **w** and bias **b**.\n",
        "- **final_cost**: The computed cost using the `compute_cost` function, representing the error between the predicted and actual values.\n",
        "- **mse**: The Mean Squared Error, calculated as **2 times the final cost**, which is a common metric to evaluate regression models.\n",
        "\n",
        "\n",
        "#### Key Metrics:\n",
        "\n",
        "- **Residual Analysis**: The residuals are the differences between the actual and predicted values. This metric is calculated by taking the mean of the absolute differences between the actual test values (**y_test**) and predicted values (**y_pred_test**). A lower value indicates a better fit.\n",
        "  \n",
        "- **Mean Absolute Error (MAE)**: MAE is the average of the absolute differences between the actual and predicted values. It provides a simple measure of prediction accuracy.\n",
        "  \n",
        "- **Mean Squared Error (MSE)**: MSE is the average of the squared differences between the actual and predicted values. It penalizes larger errors more than MAE, making it sensitive to outliers.\n",
        "\n",
        "- **Root Mean Squared Error (RMSE)**: RMSE is the square root of the MSE and provides an error metric in the same unit as the target variable, making it more interpretable.\n",
        "\n",
        "- **R-squared (R²)**: R-squared measures the proportion of variance in the target variable that is predictable from the independent variables. A value closer to 1 indicates a better fit of the model to the data."
      ],
      "metadata": {
        "id": "Da3cvWQfN_0a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYEYXdtD-E0t",
        "outputId": "85cc4777-5eaa-473c-a55d-89dcdc89b6b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Cost: 1447.3486\n",
            "Mean Squared Error: 2894.6971\n",
            "Final Weights: [  2.00680987 -11.42232329  26.52055336  16.28157195  -6.5421879\n",
            "  -4.79203362  -9.39340099   7.50916631  20.62575615   2.62898999]\n",
            "Final Bias: 151.29598957844925\n"
          ]
        }
      ],
      "source": [
        "y_pred = predict(X, w, b)\n",
        "final_cost = compute_cost(X, y, w, b)\n",
        "mse = 2 * final_cost\n",
        "\n",
        "print(f'Final Cost: {final_cost:.4f}')\n",
        "print(f'Mean Squared Error: {mse:.4f}')\n",
        "print('Final Weights:', w)\n",
        "print('Final Bias:', b)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_test = predict(X_test, w, b)\n",
        "\n",
        "# Calculate metrics\n",
        "\n",
        "print(f\"Residual Analysis : {(np.abs(y_test - y_pred_test)).mean()}\")\n",
        "\n",
        "mae = np.abs(y_test - y_pred_test).mean()\n",
        "mse = ((y_test - y_pred_test)**2).mean()\n",
        "rmse = np.sqrt(((y_test - y_pred_test)**2).sum()/len(y_test))\n",
        "\n",
        "\n",
        "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
        "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
        "\n",
        "\n",
        "SS_res = np.sum((y_test - y_pred_test)**2)        # Sum of squares of residuals(diff bet actual and predicted values)\n",
        "SS_tot = np.sum((y_test - np.mean(y_test))**2)    # Total sum of squares\n",
        "\n",
        "r2_ = 1 - (SS_res / SS_tot)\n",
        "\n",
        "print(f\"R-squared: {r2_:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rd_eyn8cfTsp",
        "outputId": "f1a67dba-b0f5-48af-e44f-073e4ab79506"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Residual Analysis : 42.902661677644126\n",
            "Mean Absolute Error (MAE): 42.9027\n",
            "Mean Squared Error (MSE): 2885.1498\n",
            "Root Mean Squared Error (RMSE): 53.7136\n",
            "R-squared: 0.4554\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0Fmgu72-E0t"
      },
      "source": [
        "# 11. Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "In this project, we successfully implemented a simple linear regression model from scratch using gradient descent. The following key steps were involved:\n",
        "\n",
        "1. **Data Preparation**: We loaded and explored the dataset, performing necessary preprocessing, such as scaling features and splitting the data into training and test sets.\n",
        "2. **Model Initialization**: The model's parameters (weights and bias) were initialized to zero.\n",
        "3. **Prediction and Cost Calculation**: We defined a prediction function and a cost function to evaluate the accuracy of the model.\n",
        "4. **Gradient Descent**: We calculated the gradients and applied gradient descent to update the model parameters iteratively.\n",
        "5. **Model Training**: Over 1000 iterations, the model was trained, and the weights and bias were updated.\n",
        "6. **Model Evaluation**: Finally, we evaluated the model's performance using cost and mean squared error (MSE), and printed the final learned parameters.\n",
        "\n",
        "### Key Findings:\n",
        "- The linear regression model was successfully trained and optimized using gradient descent.\n",
        "- The final learned weights and bias were printed, showing the results of the optimization.\n",
        "- The model's performance was assessed using the cost and MSE, providing a clear understanding of its predictive ability.\n",
        "\n",
        "### Benefits of Linear Regression:\n",
        "- **Simplicity and Interpretability**: Linear regression is easy to implement and understand, making it a great starting point for many machine learning problems.\n",
        "- **Efficiency**: The model is computationally inexpensive and can handle large datasets effectively.\n",
        "- **Well-Suited for Linearly-Related Data**: It is ideal for predicting continuous target variables that have a linear relationship with the features.\n",
        "\n",
        "### Drawbacks of Linear Regression:\n",
        "- **Assumption of Linearity**: Linear regression assumes a linear relationship between the features and target variable, which may not always hold true.\n",
        "- **Sensitivity to Outliers**: Outliers can have a significant impact on the model's performance, as linear regression is sensitive to extreme values.\n",
        "- **Multicollinearity**: High correlation between input features can lead to unreliable parameter estimates and degrade the model's performance.\n",
        "\n",
        "In conclusion, while linear regression provides an effective and interpretable model for problems with a linear relationship, its limitations in handling non-linearity and outliers must be considered. For more complex relationships, other models such as polynomial regression or more advanced machine learning algorithms may be necessary.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "_xYqGqmB2_5W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 12. Comparison with Sklearn Linear Regression"
      ],
      "metadata": {
        "id": "VKtHkyzVtfNp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we use **scikit-learn** to train a linear regression model and evaluate its performance using the test data. The model is trained on the features (**X**) and target values (**y**) and then evaluated using **Mean Squared Error (MSE)** and **R-squared (R²)** metrics.\n",
        "\n",
        "### Code Explanation:\n",
        "\n",
        "1. **Model Initialization**:\n",
        "   - `model = LinearRegression()`: Initializes a linear regression model using **scikit-learn**'s `LinearRegression` class.\n",
        "\n",
        "2. **Model Training**:\n",
        "   - `model.fit(X, y)`: Fits the model to the training data (**X**, **y**) by finding the optimal values for the weights and bias.\n",
        "\n",
        "3. **Prediction**:\n",
        "   - `y_pred_test = model.predict(X_test)`: Uses the trained model to predict the target values (**y_pred_test**) for the test data (**X_test**).\n",
        "\n",
        "4. **Model Evaluation**:\n",
        "   - `mse = mean_squared_error(y_test, y_pred_test)`: Computes the **Mean Squared Error (MSE)** between the actual target values (**y_test**) and the predicted values (**y_pred_test**). MSE measures the average squared difference between the predicted and actual values.\n",
        "   - `r2 = r2_score(y_test, y_pred_test)`: Calculates the **R-squared (R²)** score, which indicates the proportion of the variance in the target variable that is explained by the model.\n",
        "\n",
        "5. **Print Results**:\n",
        "   - The code prints the computed **MSE** and **R-squared** values, providing an understanding of how well the model fits the data."
      ],
      "metadata": {
        "id": "2VO0szeRt2U0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import SGDRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "model = SGDRegressor(\n",
        "    loss='squared_error',\n",
        "    alpha=0.0,\n",
        "    learning_rate='constant',  # Set to 'constant' for a fixed learning rate\n",
        "    eta0=0.01,                 # Initial learning rate (will remain fixed)\n",
        "    max_iter=1000,\n",
        "    random_state=42\n",
        "    # tol = None                    # no early stopping\n",
        "  )\n",
        "\n",
        "\n",
        "model.fit(X, y)  # Use training data for fitting\n",
        "\n",
        "# 4. Make predictions on the test set\n",
        "y_pred_test = model.predict(X_test)\n",
        "\n",
        "# 5. Evaluate the model\n",
        "mse = mean_squared_error(y_test, y_pred_test)\n",
        "r2 = r2_score(y_test, y_pred_test)\n",
        "\n",
        "print(f\"Mean Squared Error: {mse:.4f}\")\n",
        "print(f\"R-squared: {r2:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sOgjmtELsiJC",
        "outputId": "9fa19a31-44ae-4139-8d11-00f9b41d9093"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 2900.1936\n",
            "R-squared: 0.4526\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('scratch model weights : ' , model.coef_)\n",
        "print('sklearn model weights : ' , w)"
      ],
      "metadata": {
        "id": "fVA_eVUvuKTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bonus Model Training"
      ],
      "metadata": {
        "id": "bwTbbobP2s2W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "\n",
        "np.random.seed(42)\n",
        "X = np.random.rand(100, 1) * 10\n",
        "y = 2 * X + 5 + np.random.randn(100, 1) * 2\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Linear Regression model\n",
        "model = LinearRegression()\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Mean Squared Error: {mse:.4f}\")\n",
        "print(f\"R-squared: {r2:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "idi3QJkN1u8l",
        "outputId": "6f3572d6-1407-4e59-f56e-d09987d10f54"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 2.6148\n",
            "R-squared: 0.9287\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "HJ78tTOX-E0n",
        "PnlYUqZm-E0p",
        "9nmrg0hp-E0q",
        "WSo7mxNN-E0r",
        "VQdNE_gf-E0r",
        "y_PZqbPD-E0r",
        "huel8spq-E0s",
        "bsa2rfBf-E0s",
        "moKRi5uP-E0s",
        "Ycl4HylF-E0s",
        "2w6DM956-E0t",
        "g0Fmgu72-E0t",
        "VKtHkyzVtfNp",
        "bwTbbobP2s2W"
      ]
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}